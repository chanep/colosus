lr 0.003
--------
d13 vs d12: 258.0-142.0
d13 vs cpo: 136.5-263.5
d14 vs d13: 244.0-156.0
d14 vs cpo: 79.5-320.5
d15 vs d14: 252.5-147.5
d15 vs cpo: 79.5-320.5
d16 vs d15: 218.5-181.5
d16 vs cpo: 83.0-317.0

lr 0.001
--------
d17 vs d16: 272.0-128.0
d17 vs cpo: 118.0-282.0
d18 vs d17: 258.0-142.0
d18 vs cpo: 134.0-266.0
d19 vs d18: 235.5-164.5
d19 vs cpo: 104.5-295.5
d20 vs d19: 250.0-150.0
d20 vs cpo: 99.0-301.0
d21 vs d20: 231.5-168.5
d21 vs cpo: 108.5-291.5
d22 vs d21: 232.5-167.5
d22 vs cpo: 140.0-260.0
d23 vs d22: 204.0-196.0
d23 vs cpo: 158.0-242.0
d24 vs d23: 197.0-203.0
d24 vs cpo: 146.0-254.0

lr 0.00046
----------
d25 vs d24: 220.5-179.5
d25 vs cpo: 158.0-242.0
d26 vs d25: 234.5-165.5
d26 vs cpo: 191.0-209.0
d27 vs d26: 220.5-179.5
d27 vs cpo: 151.0-249.0
d28 vs d27: 236.0-164.0
d28 vs cpo: 120.0-280.0
d29 vs d28: 257.0-143.0
d29 vs cpo: 143.0-257.0

lr 0.0001
---------
d30 vs d29: 277.5-122.5
d30 vs cpo: 124.5-275.5
d31 vs d30: 230.5-169.5
d31 vs cpo: 141.0-259.0
d32 vs d31: 220.5-179.5
d32 vs cpo: 160.0-240.0
d33 vs d32: 230.0-170.0
d33 vs cpo: 153.0-247.0
d34 vs d33: 263.0-137.0
d34 vs cpo: 141.5-258.5

lr 0.00003
----------
d35 vs d34: 242.0-158.0
d35 vs cpo: 145.5-254.5
d36 vs d35: 268.0-132.0
d36 vs cpo: 139.5-260.5
d37 vs d36: 218.0-182.0
d37 vs cpo: 140.5-259.5
d38 vs d37: 214.5-185.5
d38 vs cpo: 139.5-260.5
d39 vs d38: 231.5-168.5
d39 vs cpo: 130.5-269.5
d40 vs d39: 249.5-150.5
d40 vs cpo: 149.5-250.5
d41 vs d40: 210.5-189.5
d41 vs cpo: 145.0-255.0

lr 0.00001
----------
d42 vs d41: 205.5-194.5
d42 vs cpo: 172.0-228.0
d43 vs d42: 207.0-193.0
d43 vs cpo: 186.0-214.0
d44 vs d43: 228.5-171.5
d44 vs cpo: 190.0-210.0
d45 vs d44: 218.0-182.0
d45 vs cpo: 174.0-226.0
d46 vs d45: 213.0-187.0
d46 vs cpo: 165.0-235.0
d47 vs d46: 201.0-199.0
d47 vs cpo: 165.5-234.5
d48 vs d47: 217.0-183.0
d48 vs cpo: 167.0-233.0
d49 vs d48: 215.5-184.5
d49 vs cpo: 172.5-227.5
d50 vs d49: 225.5-174.5
d50 vs cpo: 168.0-232.0

d4750_5ep vs cpo: 195.5-204.5

d51 vs d4750_5ep: 223.0-177.0
d51 vs cpo: 195.0-205.0
d52 vs d51: 237.5-162.5
d52 vs cpo: 211.0-189.0
d53 vs d52: 241.0-159.0
d53 vs cpo: 209.0-191.0


residual_blocks = 6, conv_size = 160
------------------------------------
e01 vs d53: 234.0-166.0
e01 vs cpo: 243.0-157.0




fpu vs no fpu: 144.5-55.5

cpuct
1.41 vs 3: 220.5-179.5

cpuct
1.41 vs 2: 98.0-110.0

cpuct
2 vs 2.5: 36.0-28.0



d_4789_2000_800.h5 0003
 - 1185s - loss: 2.0535 - policy_loss: 1.5046 - value_loss: 0.0742 - policy_acc: 0.7468 - val_loss: 2.1481 - val_policy_loss: 1.5672 - val_value_loss: 0.0906 - val_policy_acc: 0.7141

d_4789_2000_800_001.h5
 - 1195s - loss: 6.6565 - policy_loss: 4.8270 - value_loss: 0.6228 - policy_acc: 0.0288 - val_loss: 6.6621 - val_policy_loss: 4.8321 - val_value_loss: 0.6219 - val_policy_acc: 0.0287

d_4789_2000_800_0001.h5
 - 1204s - loss: 1.9414 - policy_loss: 1.4468 - value_loss: 0.0470 - policy_acc: 0.7789 - val_loss: 2.0723 - val_policy_loss: 1.5350 - val_value_loss: 0.0678 - val_policy_acc: 0.7261

d_4789_2000_800_00003.h5
 - 1191s - loss: 1.9717 - policy_loss: 1.4884 - value_loss: 0.0478 - policy_acc: 0.7607 - val_loss: 2.1095 - val_policy_loss: 1.5799 - val_value_loss: 0.0714 - val_policy_acc: 0.7098

d_4789_2000_800_0001_5ep.h5
 - 1192s - loss: 1.8548 - policy_loss: 1.3928 - value_loss: 0.0290 - policy_acc: 0.8148 - val_loss: 2.0304 - val_policy_loss: 1.5096 - val_value_loss: 0.0588 - val_policy_acc: 0.7391

d_4750_2000_800.h5
 - 1918s - loss: 1.8990 - policy_loss: 1.4234 - value_loss: 0.0417 - policy_acc: 0.7877 - val_loss: 1.9895 - val_policy_loss: 1.4794 - val_value_loss: 0.0625 - val_policy_acc: 0.7468

d_4750_2000_800_5ep.h5
val_policy_acc: 0.7540

d_4750_2000_800_8res.h5
val_policy_acc: 0.7560


training d_51_2000_800...
 - 366s - loss: 2.0655 - policy_loss: 1.5303 - value_loss: 0.0826 - policy_acc: 0.7558 - val_loss: 2.0750 - val_policy_loss: 1.5354 - val_value_loss: 0.0859 - val_policy_acc: 0.7569

pip tf 1.12
predict_on_batch time: 1.5773558616638184
predict time: 12.099738121032715 0.18905840814113617

training d_50_100_800...
22s

pip tf 1.13.1
predict_on_batch time: 1.66
predict time: 17


clr
 - 19s - loss: 2.7130 - policy_loss: 2.0222 - value_loss: 0.1348 - policy_acc: 0.6456 - val_loss: 4.0534 - val_policy_loss: 2.9940 - val_value_loss: 0.2597 - val_policy_acc: 0.3533
training finished!
Ran 1 test in 195.235s


no clr
 - 21s - loss: 3.9975 - policy_loss: 3.0121 - value_loss: 0.1979 - policy_acc: 0.3686 - val_loss: 4.6403 - val_policy_loss: 3.4695 - val_value_loss: 0.2690 - val_policy_acc: 0.2606
training finished!
Ran 1 test in 251.390s

clr triangular
training...
D:\repos\colosus\colosus\colosus_model.py:281: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  boards = np.stack(positions)
Train on 2647223 samples, validate on 54025 samples
Epoch 1/20
 - 1940s - loss: 3.2307 - policy_loss: 2.3882 - value_loss: 0.1988 - policy_acc: 0.4815 - val_loss: 2.4912 - val_policy_loss: 1.8175 - val_value_loss: 0.1497 - val_policy_acc: 0.6189
Epoch 2/20
 - 1916s - loss: 2.3000 - policy_loss: 1.6760 - value_loss: 0.1225 - policy_acc: 0.6658 - val_loss: 2.1366 - val_policy_loss: 1.5671 - val_value_loss: 0.0971 - val_policy_acc: 0.7062
Epoch 3/20
 - 1922s - loss: 2.1867 - policy_loss: 1.5989 - value_loss: 0.1026 - policy_acc: 0.6944 - val_loss: 2.2676 - val_policy_loss: 1.6417 - val_value_loss: 0.1124 - val_policy_acc: 0.6693
Epoch 4/20
 - 1921s - loss: 2.1772 - policy_loss: 1.5798 - value_loss: 0.0959 - policy_acc: 0.7002 - val_loss: 2.0743 - val_policy_loss: 1.5174 - val_value_loss: 0.0793 - val_policy_acc: 0.7274
Epoch 5/20
 - 1916s - loss: 2.1035 - policy_loss: 1.5373 - value_loss: 0.0823 - policy_acc: 0.7196 - val_loss: 2.2123 - val_policy_loss: 1.5967 - val_value_loss: 0.1044 - val_policy_acc: 0.6847
Epoch 6/20
 - 1916s - loss: 2.1369 - policy_loss: 1.5498 - value_loss: 0.0853 - policy_acc: 0.7114 - val_loss: 2.0524 - val_policy_loss: 1.5008 - val_value_loss: 0.0718 - val_policy_acc: 0.7332
Epoch 7/20
 - 1916s - loss: 2.0663 - policy_loss: 1.5116 - value_loss: 0.0724 - policy_acc: 0.7309 - val_loss: 2.1670 - val_policy_loss: 1.5702 - val_value_loss: 0.0899 - val_policy_acc: 0.6980
Epoch 8/20
 - 1916s - loss: 2.1173 - policy_loss: 1.5360 - value_loss: 0.0801 - policy_acc: 0.7172 - val_loss: 2.0428 - val_policy_loss: 1.4930 - val_value_loss: 0.0697 - val_policy_acc: 0.7341
Epoch 9/20
 - 1916s - loss: 2.0434 - policy_loss: 1.4967 - value_loss: 0.0668 - policy_acc: 0.7376 - val_loss: 2.1452 - val_policy_loss: 1.5560 - val_value_loss: 0.0861 - val_policy_acc: 0.7012
Epoch 10/20
 - 1915s - loss: 2.1055 - policy_loss: 1.5284 - value_loss: 0.0770 - policy_acc: 0.7197 - val_loss: 2.0358 - val_policy_loss: 1.4884 - val_value_loss: 0.0675 - val_policy_acc: 0.7365
Epoch 11/20
 - 1915s - loss: 2.0276 - policy_loss: 1.4871 - value_loss: 0.0628 - policy_acc: 0.7427 - val_loss: 2.1257 - val_policy_loss: 1.5444 - val_value_loss: 0.0825 - val_policy_acc: 0.7054
Epoch 12/20
 - 1915s - loss: 2.0976 - policy_loss: 1.5237 - value_loss: 0.0752 - policy_acc: 0.7215 - val_loss: 2.0339 - val_policy_loss: 1.4874 - val_value_loss: 0.0665 - val_policy_acc: 0.7349
Epoch 13/20
 - 1915s - loss: 2.0158 - policy_loss: 1.4802 - value_loss: 0.0602 - policy_acc: 0.7460 - val_loss: 2.1161 - val_policy_loss: 1.5380 - val_value_loss: 0.0823 - val_policy_acc: 0.7081
Epoch 14/20
 - 1915s - loss: 2.0906 - policy_loss: 1.5199 - value_loss: 0.0736 - policy_acc: 0.7234 - val_loss: 2.0279 - val_policy_loss: 1.4843 - val_value_loss: 0.0647 - val_policy_acc: 0.7376
Epoch 15/20
 - 1916s - loss: 2.0059 - policy_loss: 1.4746 - value_loss: 0.0581 - policy_acc: 0.7489 - val_loss: 2.1050 - val_policy_loss: 1.5296 - val_value_loss: 0.0839 - val_policy_acc: 0.7129
Epoch 16/20
 - 1915s - loss: 2.0853 - policy_loss: 1.5172 - value_loss: 0.0727 - policy_acc: 0.7244 - val_loss: 2.0329 - val_policy_loss: 1.4875 - val_value_loss: 0.0659 - val_policy_acc: 0.7350
Epoch 17/20
 - 1915s - loss: 1.9982 - policy_loss: 1.4705 - value_loss: 0.0564 - policy_acc: 0.7510 - val_loss: 2.0906 - val_policy_loss: 1.5272 - val_value_loss: 0.0742 - val_policy_acc: 0.7141
Epoch 18/20
 - 1915s - loss: 2.0808 - policy_loss: 1.5151 - value_loss: 0.0719 - policy_acc: 0.7251 - val_loss: 2.0317 - val_policy_loss: 1.4861 - val_value_loss: 0.0668 - val_policy_acc: 0.7370


